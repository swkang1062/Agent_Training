# -*- coding: utf-8 -*-
import asyncio
import os
from typing import TypedDict, Annotated, Sequence
import operator

# LangChain ê´€ë ¨ ëª¨ë“ˆ
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode

# ğŸ”½ ì‹¤ì œ ì‚¬ìš© ì¤‘ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from langchain_mcp_adapters.client import MultiServerMCPClient

# This will be initialized in the main function after loading tools
model_with_tools = None

#%%
import os

GEMINI_API_KEY = "AIzaSyBkGZrOfFB4I2V0cH1XMKP5Iaax9knvhXA"

#%%


# --- LangGraph ìƒíƒœ ì •ì˜ ---
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]



#%%

def should_continue(state: AgentState) -> str:
    """
    Determines the next step for the agent.

    Checks the last message in the state:
    - If it contains tool calls, the agent should execute the tools ("continue").
    - Otherwise, the conversation can end ("end").
    - It also includes a safety stop after 10 iterations to prevent infinite loops.
    """
    last_message = state['messages'][-1]
    print("state['messages'] === ", state['messages'])

    if len(state['messages']) > 10:
        print("--- ğŸš¦ Max iterations reached. Ending conversation. ---")
        return "end"

    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        # Correctly check for non-empty tool_calls list
        if last_message.tool_calls:
             print(f"--- ğŸ“ Decision: Call tool(s): {[tool['name'] for tool in last_message.tool_calls]} ---")
             return "continue"

    print("--- âœ… Decision: End conversation and respond to user. ---")
    return "end"

async def call_model(state: AgentState):
    """
    Calls the LLM with the current conversation history to decide the next action
    or generate a final response.
    """
    print("--- ğŸ§  Calling LLM... ---")
    messages = state['messages']
    
    print("messages ============= ", messages)
    # The model (bound with tools) will either respond directly or request a tool call
    if model_with_tools:
        response = await model_with_tools.ainvoke(messages)
        return {"messages": [response]}
    else:
        raise ValueError("Model with tools is not initialized.")
#%%

# --- Main Asynchronous Function ---
async def main():
    """
    Sets up and runs the LangGraph agent.
    """
    # 1. Validate API Key
    if not GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY environment variable not set. Please set it before running.")

    # 2. Initialize the Language Model
    global model_with_tools
    model = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=GEMINI_API_KEY)
    print("ğŸ¤– Google Gemini client initialized.")

    # 3. Initialize the MCP Client to connect to the tool server
    # This client will manage the connection and tool discovery.
    client = MultiServerMCPClient({
        "weather_server": {
            "transport": "streamable_http",
            "url": "http://localhost:8080/mcp"
        }
    })

    # 4. Fetch tools from the remote server
    print("ğŸ”— Connecting to server to fetch tools...")
    try:
        tools = await client.get_tools()
        if not tools:
            print("âš ï¸ No tools found on the server. Please ensure the server is running and provides tools.")
            return
        print(f"âœ… Tools loaded successfully: {[tool.name for tool in tools]}")
    except Exception as e:
        print(f"âŒ Error connecting to server or loading tools: {e}")
        print("   Please ensure the 'weather_mcp_server.py' is running on http://localhost:8080/mcp.")
        return

    # 5. Bind the fetched tools to the model
    # This allows the model to know which tools it can call.
    model_with_tools = model.bind_tools(tools)

    # 6. Define the LangGraph workflow
    workflow = StateGraph(AgentState)

    # 6a. Define nodes
    # 'agent': The node that calls the LLM.
    # 'action': The node that executes the tools.
    workflow.add_node("agent", call_model)
    workflow.add_node("action", ToolNode(tools))

    # 6b. Define edges
    # The entry point is the 'agent' node.
    workflow.set_entry_point("agent")

    # Add a conditional edge from 'agent'. Based on the `should_continue` function,
    # it will either go to 'action' (if a tool is called) or end the graph.
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "continue": "action",
            "end": END,
        },
    )

    # After an action is performed, the graph always goes back to the 'agent' node.
    workflow.add_edge("action", "agent")

    # 6c. Compile the graph into a runnable application
    app = workflow.compile()
    print("âœ… LangGraph workflow compiled successfully.")

    # --- Interactive Chat Loop ---
    print("\nğŸ¤– ì œì£¼ë„ ì‚¬íˆ¬ë¦¬ ë‚ ì”¨ ì—ì´ì „íŠ¸ 'ë‚ ì”¨ë´‡'ì´ë¼ê½ˆ. ë¬´ì‹ ê±° ë¬¼ì–´ë³´ì¿ ê´‘? (ì¢…ë£Œ: 'quit')")
    

    # --- ì‹¤í–‰ ì½”ë“œ ---
    async def run_agent(query: str):
        """LangGraph ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜"""
        system_message = (
        "ë‹¹ì‹ ì€ ë‚ ì”¨ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ëŠ” ì „ë¬¸ ì—ì´ì „íŠ¸ 'ë‚ ì”¨ë´‡'ì…ë‹ˆë‹¤. "
        "ì£¼ì–´ì§„ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤. "
        "ëª¨ë“  ë‹¨ê³„ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê³ , ìµœì¢… ë‹µë³€ì€ ë°˜ë“œì‹œ **ì œì£¼ë„ ì‚¬íˆ¬ë¦¬**ë¡œ, "
        "ì •ê° ìˆê³  ì¹œì ˆí•˜ê²Œ ë§ˆë¬´ë¦¬í•´ì£¼ì„¸ìš”. (ì˜ˆ: ~í–ˆìˆ˜ë‹¤, ~ê½ˆ?, ~ê±¸ë§ˆì”¸)"
        )
        # HumanMessageë§Œ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì • (ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ì²«ë²ˆì§¸ HumanMessageì— í¬í•¨)
        initial_messages = [HumanMessage(content=f"{system_message}\n\nì§ˆë¬¸: {query}")]

        final_state = await app.ainvoke({"messages": initial_messages})
        final_answer = final_state['messages'][-1]
        print("\n" + "="*50)
        print("âœ… ìµœì¢… ë‹µë³€")
        print("="*50)
        print(final_answer.content)


    print("\nğŸ¤– ì œì£¼ë„ ì‚¬íˆ¬ë¦¬ ë‚ ì”¨ ì—ì´ì „íŠ¸ 'ë‚ ì”¨ë´‡'ì´ë¼ê½ˆ. ë¬´ì‹ ê±° ë¬¼ì–´ë³´ì¿ ê´‘? (ì¢…ë£Œ: 'quit')")
    while True:
        user_input = input("\nğŸ‘¤ ì§ˆë¬¸: ")
        if user_input.lower() in ["quit", "exit", "ì¢…ë£Œ"]:
            print("ğŸ‘‹ ì•ˆë…•íˆ ê°€ì‹­ì„œê²Œ.")
            break
        if not user_input:
            continue
        await run_agent(user_input)

#%%


# --- ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    # ë¹„ë™ê¸° í•¨ìˆ˜ì¸ main()ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    asyncio.run(main())

